{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TENG Gait Analysis: ML Pipeline\n",
    "\n",
    "This notebook covers the complete machine learning pipeline for the TENG gait sensor project, from data loading to model training and evaluation.\n",
    "\n",
    "**Goals:**\n",
    "1.  **Activity Classification:** (4 classes: stand, walk, run, jump)\n",
    "2.  **Clinical QOM Assessment:** (3 classes: Intervention Priority, Improvement Recommended, Healthy Baseline)\n",
    "3.  **Bio-authentication:** (Regression: Predict weight)\n",
    "4.  **Clinical Metrics:** (Activity Duration, QOM Confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Data Loading\n",
    "\n",
    "This section handles library imports and loading the dataset.\n",
    "\n",
    "**For Google Colab:**\n",
    "1.  Upload your `User_Gait_Data_Master.zip` to your Google Drive.\n",
    "2.  Run the first cell and authorize Google Drive.\n",
    "3.  The second cell will unzip the data into your Colab environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GroupShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn.metrics import classification_report, confusion_matrix, mean_absolute_error, r2_score\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import warnings\n",
    "\n",
    "# Suppress common warnings\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "else:\n",
    "    print(\"Not running in Google Colab. Assuming local setup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Data Paths ---\n",
    "# Base path for the data\n",
    "if IN_COLAB:\n",
    "    # Unzip the data from Google Drive to the Colab environment\n",
    "    print(\"Unzipping data from Google Drive...\")\n",
    "    !unzip -q -o \"/content/drive/My Drive/User_Gait_Data_Master.zip\" -d \"/content/\"\n",
    "    DATA_DIR = Path(\"/content/User_Gait_Data_Master\")\n",
    "else:\n",
    "    # Local path\n",
    "    DATA_DIR = Path(\"User_Gait_Data_Master\")\n",
    "\n",
    "SIGNALS_DIR = DATA_DIR / \"User_Data_Labelled\"\n",
    "LABELS_FILE = DATA_DIR / \"QOM\" / \"gait_labels_qom.csv\"\n",
    "\n",
    "print(f\"Data directory set to: {DATA_DIR}\")\n",
    "print(f\"Signals directory set to: {SIGNALS_DIR}\")\n",
    "print(f\"Labels file set to: {LABELS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master labels file\n",
    "try:\n",
    "    labels_df = pd.read_csv(LABELS_FILE)\n",
    "    print(f\"Successfully loaded labels file with {len(labels_df)} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Labels file not found at {LABELS_FILE}\")\n",
    "    print(\"Please check your file path and ensure you have unzipped the data.\")\n",
    "\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Visualization (GUI Equivalent)\n",
    "\n",
    "As requested, here is a simple visualization function to plot any signal from the dataset. This acts as a simple 'GUI' to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal_by_name(file_name):\n",
    "    \"\"\"Loads and plots a signal CSV given its file_name from the labels_df.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Find the full path and metadata\n",
    "        metadata = labels_df[labels_df['file_path'] == file_name].iloc[0]\n",
    "        file_path = SIGNALS_DIR / file_name\n",
    "        \n",
    "        # Load the signal data\n",
    "        signal_df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get activity duration\n",
    "        duration = signal_df['Time(s)'].max() - signal_df['Time(s)'].min()\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.plot(signal_df['Time(s)'], signal_df['Voltage(V)'])\n",
    "        plt.title(f\"Signal for: {file_name} (Activity: {metadata['activity']})\")\n",
    "        plt.xlabel(\"Time (s)\")\n",
    "        plt.ylabel(\"Voltage (V)\")\n",
    "        plt.text(0.01, 0.9, f\"Subject: {metadata['subject_id']}\", transform=plt.gca().transAxes)\n",
    "        plt.text(0.01, 0.8, f\"QOM: {metadata['qom']}\", transform=plt.gca().transAxes)\n",
    "        plt.text(0.01, 0.7, f\"Duration: {duration:.2f}s\", transform=plt.gca().transAxes)\n",
    "        plt.grid(True, linestyle=':')\n",
    "        plt.show()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Signal file not found at {file_path}\")\n",
    "    except IndexError:\n",
    "        print(f\"ERROR: File name '{file_name}' not found in labels file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- Example Plots ---\n",
    "print(\"Plotting example signals for each activity...\")\n",
    "\n",
    "# Find one example for each activity\n",
    "plot_signal_by_name(labels_df[labels_df['activity'] == 'stand']['file_path'].iloc[0])\n",
    "plot_signal_by_name(labels_df[labels_df['activity'] == 'walk']['file_path'].iloc[0])\n",
    "plot_signal_by_name(labels_df[labels_df['activity'] == 'run']['file_path'].iloc[0])\n",
    "plot_signal_by_name(labels_df[labels_df['activity'] == 'jump']['file_path'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "This is the most critical step. We will read every CSV file and extract a set of statistical features to represent the *entire* signal with a *single* row of data. This is what the models will be trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_path):\n",
    "    \"\"\"Extracts a set of features from a single signal CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        v = df['Voltage(V)']\n",
    "        t = df['Time(s)']\n",
    "        \n",
    "        if v.empty:\n",
    "            return None\n",
    "        \n",
    "        features = {\n",
    "            'v_mean': v.mean(),\n",
    "            'v_std': v.std(),\n",
    "            'v_max': v.max(),\n",
    "            'v_min': v.min(),\n",
    "            'v_range': v.max() - v.min(),\n",
    "            'v_abs_mean': v.abs().mean(),\n",
    "            'v_skew': v.skew(),\n",
    "            'v_kurt': v.kurt(),\n",
    "            'duration': t.max() - t.min()\n",
    "            # Add more features here (e.g., from scipy.stats)\n",
    "        }\n",
    "        return features\n",
    "        \n",
    "    except pd.errors.EmptyDataError:\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over all files in the labels_df and apply feature extraction\n",
    "print(\"Starting feature extraction for 84 files...\")\n",
    "\n",
    "all_features = []\n",
    "for file_name in labels_df['file_path']:\n",
    "    file_path = SIGNALS_DIR / file_name\n",
    "    features = extract_features(file_path)\n",
    "    \n",
    "    if features:\n",
    "        all_features.append(features)\n",
    "    else:\n",
    "        print(f\"Warning: Could not process {file_name}\")\n",
    "        all_features.append({}) # Append empty dict to maintain index\n",
    "\n",
    "# Create a DataFrame from the features\n",
    "features_df = pd.DataFrame(all_features)\n",
    "\n",
    "# Combine features with labels\n",
    "# We reset index to ensure a clean join\n",
    "ml_df = pd.concat([labels_df.reset_index(drop=True), features_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Add BMI\n",
    "ml_df['bmi'] = ml_df['weight_kg'] / ((ml_df['height_cm'] / 100) ** 2)\n",
    "\n",
    "# Map QOM to 3-class system\n",
    "def map_qom(qom_score):\n",
    "    if qom_score <= 3:\n",
    "        return 'Intervention Priority' # Class 0\n",
    "    elif qom_score <= 6:\n",
    "        return 'Improvement Recommended' # Class 1\n",
    "    else:\n",
    "        return 'Healthy Baseline' # Class 2\n",
    "\n",
    "ml_df['qom_3_class'] = ml_df['qom'].apply(map_qom)\n",
    "\n",
    "print(f\"Feature extraction complete. Created DataFrame with shape: {ml_df.shape}\")\n",
    "ml_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training: Activity Classification (4-Class)\n",
    "\n",
    "**Goal:** Predict `activity` (stand, walk, run, jump) from sensor features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model 1: Activity Classification (4-Class) ---\")\n",
    "\n",
    "# 1. Define Features (X) and Target (y)\n",
    "# We include demographic features as they might be relevant (e.g., weight affects jump signal)\n",
    "feature_cols = ['v_mean', 'v_std', 'v_max', 'v_min', 'v_range', 'v_abs_mean', \n",
    "                'v_skew', 'v_kurt', 'duration', 'bmi']\n",
    "target_col = 'activity'\n",
    "\n",
    "# Handle missing data (e.g., from failed feature extraction or missing BMI)\n",
    "model_data = ml_df[feature_cols + [target_col, 'subject_id']].dropna()\n",
    "\n",
    "X = model_data[feature_cols]\n",
    "y = model_data[target_col]\n",
    "\n",
    "# Encode target labels (e.g., 'walk' -> 1)\n",
    "le_activity = LabelEncoder()\n",
    "y_encoded = le_activity.fit_transform(y)\n",
    "activity_classes = le_activity.classes_\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Train/Test Split (Using GroupShuffleSplit to keep subjects separate)\n",
    "# This ensures data from one subject isn't in both train and test sets.\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_scaled, y_encoded, groups=model_data['subject_id']))\n",
    "\n",
    "X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "print(f\"Data split: {len(y_train)} train, {len(y_test)} test samples.\")\n",
    "print(f\"Test subjects: {np.unique(model_data['subject_id'].iloc[test_idx])}\")\n",
    "\n",
    "# 3. Train Base Model: Random Forest\n",
    "print(\"\\nTraining Random Forest...\")\n",
    "rf_activity = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_activity.fit(X_train, y_train)\n",
    "y_pred_rf = rf_activity.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Results:\")\n",
    "print(classification_report(y_test, y_pred_rf, target_names=activity_classes))\n",
    "\n",
    "# 4. Train Advanced Model: XGBoost\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "xgb_activity = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_activity.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_activity.predict(X_test)\n",
    "\n",
    "print(\"XGBoost Results:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=activity_classes))\n",
    "\n",
    "# 5. Plot Confusion Matrix (for better model)\n",
    "cm = confusion_matrix(y_test, y_pred_xgb, normalize='true')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Blues', xticklabels=activity_classes, yticklabels=activity_classes)\n",
    "plt.title(\"Activity Classification Confusion Matrix (XGBoost)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training: Clinical QOM Assessment (3-Class)\n",
    "\n",
    "**Goal:** Predict `qom_3_class` (Intervention Priority, Improvement Recommended, Healthy Baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model 2: QOM Assessment (3-Class) ---\")\n",
    "\n",
    "# 1. Define Features (X) and Target (y)\n",
    "# We will only use 'walk', 'run', and 'jump' data, as 'stand' has no QOM.\n",
    "qom_data = ml_df[ml_df['activity'].isin(['walk', 'run', 'jump'])]\n",
    "\n",
    "feature_cols = ['v_mean', 'v_std', 'v_max', 'v_min', 'v_range', 'v_abs_mean', \n",
    "                'v_skew', 'v_kurt', 'duration', 'bmi']\n",
    "target_col = 'qom_3_class'\n",
    "\n",
    "model_data = qom_data[feature_cols + [target_col, 'subject_id']].dropna()\n",
    "\n",
    "X = model_data[feature_cols]\n",
    "y = model_data[target_col]\n",
    "\n",
    "# Encode target labels\n",
    "le_qom = LabelEncoder()\n",
    "y_encoded = le_qom.fit_transform(y)\n",
    "qom_classes = le_qom.classes_\n",
    "\n",
    "# Scale features\n",
    "scaler_qom = StandardScaler()\n",
    "X_scaled = scaler_qom.fit_transform(X)\n",
    "\n",
    "# 2. Train/Test Split (GroupShuffleSplit)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_scaled, y_encoded, groups=model_data['subject_id']))\n",
    "\n",
    "X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "y_train, y_test = y_encoded[train_idx], y_encoded[test_idx]\n",
    "\n",
    "print(f\"Data split: {len(y_train)} train, {len(y_test)} test samples.\")\n",
    "print(f\"Test subjects: {np.unique(model_data['subject_id'].iloc[test_idx])}\")\n",
    "\n",
    "# 3. Train Model: XGBoost (better for this task)\n",
    "print(\"\\nTraining XGBoost for QOM...\")\n",
    "xgb_qom = XGBClassifier(n_estimators=100, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_qom.fit(X_train, y_train)\n",
    "y_pred_xgb = xgb_qom.predict(X_test)\n",
    "\n",
    "print(\"XGBoost QOM Results:\")\n",
    "print(classification_report(y_test, y_pred_xgb, target_names=qom_classes))\n",
    "\n",
    "# 4. Clinical Confidence Rating\n",
    "print(\"\\n--- Clinical Confidence Rating ---\")\n",
    "y_pred_proba = xgb_qom.predict_proba(X_test)\n",
    "\n",
    "# Show confidence for the first 5 test samples\n",
    "for i in range(min(5, len(y_test))):\n",
    "    true_label = qom_classes[y_test[i]]\n",
    "    pred_label = qom_classes[y_pred_xgb[i]]\n",
    "    confidence = y_pred_proba[i].max() * 100\n",
    "    \n",
    "    print(f\"Sample {i}: True='{true_label}', Predicted='{pred_label}', Confidence={confidence:.2f}%\")\n",
    "\n",
    "# 5. Plot Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred_xgb, normalize='true')\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='.2f', cmap='Greens', xticklabels=qom_classes, yticklabels=qom_classes)\n",
    "plt.title(\"Clinical QOM Classification Confusion Matrix (XGBoost)\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Training: Bio-Authentication (Weight Regression)\n",
    "\n",
    "**Goal:** Predict `weight_kg` (a continuous value) from sensor features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Model 3: Bio-Authentication (Weight Regression) ---\")\n",
    "\n",
    "# 1. Define Features (X) and Target (y)\n",
    "# We can use all activities for this\n",
    "feature_cols = ['v_mean', 'v_std', 'v_max', 'v_min', 'v_range', 'v_abs_mean', \n",
    "                'v_skew', 'v_kurt', 'duration'] # Don't use BMI, as it's derived from weight\n",
    "target_col = 'weight_kg'\n",
    "\n",
    "model_data = ml_df[feature_cols + [target_col, 'subject_id']].dropna()\n",
    "\n",
    "X = model_data[feature_cols]\n",
    "y = model_data[target_col]\n",
    "\n",
    "# Scale features\n",
    "scaler_reg = StandardScaler()\n",
    "X_scaled = scaler_reg.fit_transform(X)\n",
    "\n",
    "# 2. Train/Test Split (GroupShuffleSplit)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, test_idx = next(gss.split(X_scaled, y, groups=model_data['subject_id']))\n",
    "\n",
    "X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "print(f\"Data split: {len(y_train)} train, {len(y_test)} test samples.\")\n",
    "\n",
    "# 3. Train Model: Random Forest Regressor\n",
    "print(\"\\nTraining Random Forest Regressor for Weight...\")\n",
    "rf_weight = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_weight.fit(X_train, y_train)\n",
    "y_pred = rf_weight.predict(X_test)\n",
    "\n",
    "# 4. Evaluate Regression Model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Weight Prediction Results:\")\n",
    "print(f\"  Mean Absolute Error (MAE): {mae:.2f} kg\")\n",
    "print(f\"  R-squared (R2) Score:    {r2:.2f}\")\n",
    "\n",
    "# 5. Plot Results\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, edgecolors='k')\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "plt.title(f\"Weight Prediction (MAE: {mae:.2f} kg)\")\n",
    "plt.xlabel(\"Actual Weight (kg)\")\n",
    "plt.ylabel(\"Predicted Weight (kg)\")\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Future Work & Rehabilitation Focus\n",
    "\n",
    "**Goal:** Analyze feature importance to understand links between signals and muscular weakness.\n",
    "\n",
    "We can inspect the 'feature importances' from our trained models. For example, which features did the QOM model use most? If it relies heavily on `v_max` and `v_range` during 'jump' signals to identify 'Intervention Priority' cases, this suggests a link between low signal amplitude (weaker force) and poor QOM, which could imply quadriceps weakness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Feature Importance for QOM Model (XGBoost) ---\")\n",
    "\n",
    "# Get feature importances from the trained QOM model\n",
    "importances = xgb_qom.feature_importances_\n",
    "\n",
    "# Create a DataFrame for easy plotting\n",
    "feature_imp_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': importances\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(feature_imp_df)\n",
    "\n",
    "# Plot the importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_imp_df, palette='viridis')\n",
    "plt.title('Feature Importance for Clinical QOM Classification')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsight: The model's reliance on features like 'v_std' (signal variability) and 'v_range' (peak force)\\n\")\n",
    "print(\"can be analyzed for specific activities (like 'jump') to guide hypotheses about muscular weakness.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

